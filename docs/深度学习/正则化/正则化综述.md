# 正则化综述

## 1、参数约束正则化：L1和L2正则化

参数约束正则化主要是指 $L_1$ 和 $L_2$ 正则化，关于这两个正则化，详见：[L1和L2正则化](../../正则化/L1和L2正则化/)；

## 2、Dropout

1. dropout：在前向传播过程中，对网络中的每个隐藏层，每个隐单元都以一定的概率 $p_{drop}$ 被删除，之后得到一个比原始网络要小的裁剪网络。在反向传播过程中，仅仅对该裁剪网络进行梯度更新，被删除的隐单元不进行梯度更新；

2. 对隐单元的删除指的是：将该隐单元的输出置为0；当其输出为0时，该隐单元对后续神经元的影响均为0；

3. 输入层和输出层的神经元不会被删除，这两层的神经元的个数是固定的；

4. 关于隐单元删除时的一些细节：

    * 不同的样本，其删除的隐单元的集合是不同的，因此得到的裁剪网络也是不同的；
    * 不同的样本，每个隐单元被删除的概率是相同的；
    * 同一条样本，分两次进入模型训练，那么这两次删除的隐单元也是不同的；
    * 在每个梯度更新周期中，被删除的隐单元的集合是不同的，因此得到的裁剪网络也是不同的；
    * 在每个梯度更新周期中，隐单元被删除的概率是相同的；

5. dropout仅仅用于神经网络的训练阶段，在推理阶段不需要删除隐单元，而是使用所有的神经元；

6. dropout的优点：

    * 其不限神经网络的网络结构，都可以使用该技术；
    * 其计算非常方便、高效；具体计算过程为：每次产生 $n$ 个随机的二进制数（0和1），然后将这些产生的随机二进制数与 $n$ 个隐单元的输出相乘（与0相乘的隐单元就相当于是被删除了）；这个计算开销相比于神经网络的正常计算是非常小的；

7. dropout的缺点：

    * 损失函数 Loss 不再被明确的定义，每次迭代都会随机删除一部分隐单元；

8. dropout使用时的策略：由于dropout的作用是防止过拟合，所以

    * 若某一隐层神经元比较少，过拟合不严重，可以调小概率 $p_{drop}$，甚至该层不使用 dropout；
    * 若某一隐层神经元比较多，过拟合严重，可以调大概率 $p_{drop}$；

9. dropout在使用时更多的可能性（也可称作推广）：

    * 可以对某一层或某几层使用dropout，其他层不使用dropout；
    * 可以对不同的层，甚至不同的隐单元分别设置不同的概率 $p_{drop}$，但实际中一般不这么做；
    * 可以对每个梯度更新周期设置不同的概率 $p_{drop}$，但实际中一般不这么做；
    * 现在对每个隐单元的输出乘的是个二值函数（0或1），不一定必须是二值函数，也可以扩展为乘以其他值，比如：对 $n$ 个隐单元，每个隐单元乘上的值是根据均值为0，方差为1的标准正态分布生成的值。此时单从形式上来看和 $L_1$、$L_2$ 正则已经非常相似了；不过实际中一般也很少采用这种方式；

## 3、数据增强（EDA）

1. 词汇增强：

    * 基于词典进行替换：从文本中随机选取一个或多个词语，利用同义词词典将这一个或多个词语替换成其同义词；
    
    * 基于词向量进行替换：从文本中随机选取一个或多个词语，使用预先训练好的词向量，找到在嵌入空间中与这一个或多个词距离最近的词语，进行替换；
    
    * 基于MLM进行替换：BERT系列模型是通过MLM进行预训练的，所以可以随机将文本中的某一个或者多个词语mask，使用预训练好的BERT系列模型对mask掉的词语进行生成，以此做增强；
    
    * 基于tf-idf进行替换：文本中tf-idf较小的词语，对该文本的贡献较小。可以优先对这些词语进行增强，以避免模型错误的将这些词语作为具体任务的主要判断依据；
    
    * 随机插入词语：向文本的任意位置以一定的概率随机插入词语；

    * 随机删除词语：对文本中任意词语以一定的概率随机进行删除；

    * 随机替换词语：将文本中任意的某一个或多个词语，随机的替换为另外的随机词语；

2. 回译增强：

    利用机器翻译模型，或百度翻译、谷歌翻译等，将中文文本翻译为其他语种的文本，然后再翻译回中文文本，以此进行增强；

3. 基于语法解析增强：首先使用语法解析工具解析并生成原始文本的依存关系树，之后按照依存关系对其调整，生成增强后的文本。比如：将文本从一个主动语态调整为被动语态；

## 4、早停（early stopping）

待补充；
