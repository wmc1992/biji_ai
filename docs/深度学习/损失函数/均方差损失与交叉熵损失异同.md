# 均方差损失与交叉熵损失异同

> 说明：本文讨论的是在做分类任务时这两个损失的区别；

## 一、两者概念上的区别

1、均方差损失(cross-entropy)：是求一个batch中n个样本的n个输出与其期望输出的差的平方的均值；

2、交叉熵损失(MSE)：用来评估当前训练得到的概率分布与真实分布的差异情况，它刻画的是实际输出（概率）与期望输出（概率）的距离，也就是交叉熵的值越小，两个概率分布就越接近；

## 二、两者更新速度上的区别

下面分别对均方差损失、二分类交叉熵损失、多分类交叉熵损失，推导其梯度下降的过程，并对其在梯度下降过程中优化的速度进行分析。

### 2.1 均方差损失

#### 2.1.1 问题描述

假设模型为单个神经元，单输入，单输出，二分类任务，使用sigmoid做二分类，则其前向传播过程为：

$$z_i=wx_i+b$$

$$\hat{y}_i=\sigma(z_i)$$

其损失为：

$$L=\frac{1}{N} \sum_{i=1}^N L_i=\frac{1}{N} \sum_{i=1}^N \frac{(y_i - \hat{y}_i)^2}{2}$$

符号说明：

* $N$：表示样本数量；
* $x_i$：表示第$i$条样本的输入；
* $y_i$：表示第$i$条样本的期望输出；
* $\hat{y_i}$：表示第$i$条样本的真实输出；
* $z_i$：表示第$i$条样本只经过权重矩阵，未经过激活函数的中间结果；
* $L_i$：表示第$i$条样本的损失；
* $L$：表示所有$N$条样本的损失；
* $w$和$b$：表示权重举证；

#### 2.1.2 求导

{% raw %}
$$
\begin{equation}
\begin{split}
\frac{\partial L_i}{\partial w} &= \frac{\partial L_i}{\partial \hat{y}_i} \frac{\partial \hat{y}_i}{\partial z_i} \frac{\partial z_i}{\partial w}\\
&= (\hat{y}_i - y_i) \sigma^{\prime}(z_i) x_i \\
&= (\hat{y}_i - y_i) \sigma(z_i)(1 - \sigma(z_i)) x_i \\
&= (\hat{y}_i - y_i) \hat{y}_i (1 - \hat{y}_i) x_i
\end{split}
\end{equation}
$$
{% endraw %}

{% raw %}
$$
\begin{equation}
\begin{split}   
\frac{\partial L_i}{\partial b} &= \frac{\partial L_i}{\partial \hat{y}_i} \frac{\partial \hat{y}_i}{\partial z_i} \frac{\partial z_i}{\partial b}\\
&= (\hat{y}_i - y_i) \sigma^{\prime}(z_i) \\
&= (\hat{y}_i - y_i) \sigma(z_i)(1 - \sigma(z_i)) \\
&= (\hat{y}_i - y_i) \hat{y}_i (1 - \hat{y}_i)
\end{split}
\end{equation}
$$
{% endraw %}

> 说明：上述推导中使用到了sigmoid的求导公式，若 $f(z)=\frac{1}{1+e^{-z}}$，则有：$f^{\prime}(z) = f(z)(1 - f(z))$

求解出导数之后，权重值的更新比较简单，如下：

$$w=w - \eta \frac{\partial L}{\partial w}=w - \eta (\frac{1}{N} \sum_{i=1}^N \frac{\partial L_i}{\partial w})$$

$$b=b - \eta \frac{\partial L}{\partial b}=b - \eta (\frac{1}{N} \sum_{i=1}^N \frac{\partial L_i}{\partial b})$$

#### 2.1.3 优化速度分析

以对权重值 $w$ 的更新进行分析，权重值 $b$ 的更新速度分析方式完全相同；

再重新写一下更新权重时的公式，将梯度公式 $\frac{\partial L_i}{\partial w} = (\hat{y}_i - y_i) \sigma^{\prime}(z_i) x_i$ 代入到权重更新的公式，可以得到下式：

$$
\begin{equation}
\begin{split}   
w&=w - \eta (\frac{1}{N} \sum_{i=1}^N \frac{\partial L_i}{\partial w}) \\
&= w - \eta \Big[\frac{1}{N} \sum_{i=1}^N (\hat{y}_i - y_i) \sigma^{\prime}(z_i) x_i \Big]
\end{split}
\end{equation}
$$

在上述公式中，$\eta$ 是超参数，$N$ 为样本数量，$x_i$ 为输入的值，这几个参数都不需要考虑；

所以每次对权重更新的步长主要由 $(\hat{y}_i - y_i) \sigma^{\prime}(z_i)$ 决定。

其中 $\sigma^{\prime}(z_i)$ 为 Sigmoid 函数的导数，下图是 Sigmoid 函数的图像，可以看出：当 $z_i$ 比较小或者比较大时，$\sigma^{\prime}(z_i)$ 的值（即下图曲线的斜率）都趋于0；也就是说当 $z_i$ 比较小或者比较大时，其梯度也是趋于0的。

而在做梯度下降时，希望当离目标较远时，每次更新的步长要大一些，可以快速收敛；当离目标较近时，每次更新的步长要小一些，可以防止在目标值附近震荡；

**综上所述**：在使用均方差损失时，当 $z_i$ 比较小或者比较大时，其梯度都是趋于0的。所以当梯度较小时，无法判断距离目标点较远，还是较近，优化起来比较困难。

![](/resource/nlp_basis/Loss/均方差损失和交叉熵损失_003/01.png)

### 2.2 二分类交叉熵损失

常用的交叉熵损失又分为`二分类的交叉熵损失`（sigmoid）和`多分类的交叉熵损失`（softmax），这一部分先看二分类交叉熵损失，下一段再看多分类交叉熵损失；

#### 2.2.1 问题描述

假设模型为单个神经元，单输入，单输出，二分类任务，使用sigmoid做二分类，则其前向传播过程为：

$$z_i=wx_i+b$$

$$\hat{y}_i=\sigma(z_i)$$

其损失为：

$$L_i=-\Big[ y_i \log \hat{y}_i + (1-y_i)\log (1-\hat{y}_i) \Big]$$

$$L=\frac{1}{N} \sum_{i=1}^N L_i$$

符号说明：

* $N$：表示样本数量；
* $x_i$：表示第$i$条样本的输入；
* $y_i$：表示第$i$条样本的期望输出；
* $\hat{y_i}$：表示第$i$条样本的真实输出；
* $z_i$：表示第$i$条样本只经过权重矩阵，未经过激活函数的中间结果；
* $L_i$：表示第$i$条样本的损失；
* $L$：表示所有$N$条样本的损失；
* $w$和$b$：表示权重举证；

#### 2.2.2 求导


{% raw %}
$$
\begin{equation}
\begin{split}
\frac{\partial L_i}{\partial w} &= \frac{\partial L_i}{\partial \hat{y}_i} \frac{\partial \hat{y}_i}{\partial z_i} \frac{\partial z_i}{\partial w} \\
&= -(\frac{y_i}{\hat{y}_i} - \frac{1-y_i}{1-\hat{y}_i}) \sigma^{\prime}(z_i) x_i \\
&= -(\frac{y_i}{\hat{y}_i} - \frac{1-y_i}{1-\hat{y}_i}) \sigma(z_i)(1-\sigma(z_i)) x_i \\
&= -(\frac{y_i}{\hat{y}_i} - \frac{1-y_i}{1-\hat{y}_i}) \hat{y}_i(1-\hat{y}_i) x_i \\
&= -[y_i(1-\hat{y}_i) - \hat{y}_i(1-y_i)]x_i \\
&= - (y_i - y_i \hat{y}_i - \hat{y}_i + y_i \hat{y}_i) x_i \\
&= (\hat{y}_i - y_i) x_i
\end{split}
\end{equation}
$$
{% endraw %}

{% raw %}
$$
\begin{equation}
\begin{split}
\frac{\partial L_i}{\partial b} &= \frac{\partial L_i}{\partial \hat{y}_i} \frac{\partial \hat{y}_i}{\partial z_i} \frac{\partial z_i}{\partial b} \\
&= -(\frac{y_i}{\hat{y}_i} - \frac{1-y_i}{1-\hat{y}_i}) \sigma^{\prime}(z_i) \\
&= -(\frac{y_i}{\hat{y}_i} - \frac{1-y_i}{1-\hat{y}_i}) \sigma(z_i)(1-\sigma(z_i)) \\
&= -(\frac{y_i}{\hat{y}_i} - \frac{1-y_i}{1-\hat{y}_i}) \hat{y}_i(1-\hat{y}_i) \\
&= -[y_i(1-\hat{y}_i) - \hat{y}_i(1-y_i)] \\
&= - (y_i - y_i \hat{y}_i - \hat{y}_i + y_i \hat{y}_i) \\
&= (\hat{y}_i - y_i)
\end{split}
\end{equation}
$$
{% endraw %}

> 说明：上述推导中使用到了sigmoid的求导公式，若 $f(z)=\frac{1}{1+e^{-z}}$，则有：$f^{\prime}(z) = f(z)(1 - f(z))$

求解出导数之后，权重值的更新比较简单，如下：

$$w=w - \eta \frac{\partial L}{\partial w}=w - \eta (\frac{1}{N} \sum_{i=1}^N \frac{\partial L_i}{\partial w})$$

$$b=b - \eta \frac{\partial L}{\partial b}=b - \eta (\frac{1}{N} \sum_{i=1}^N \frac{\partial L_i}{\partial b})$$

#### 2.2.3 优化速度分析

以对权重值 $w$ 的更新进行分析，权重值 $b$ 的更新速度分析方式完全相同；

再重新写一下更新权重时的公式，将梯度公式 $\frac{\partial L_i}{\partial w} = (\hat{y}_i - y_i) x_i$ 代入到权重更新的公式，可以得到下式：

$$
\begin{equation}
\begin{split}   
w&=w - \eta (\frac{1}{N} \sum_{i=1}^N \frac{\partial L_i}{\partial w}) \\
&= w - \eta \Big[\frac{1}{N} \sum_{i=1}^N (\hat{y}_i - y_i)  x_i \Big]
\end{split}
\end{equation}
$$

在上述公式中，$\eta$ 是超参数，$N$ 为样本数量，$x_i$ 为输入的值，这几个参数都不需要考虑；

所以每次对权重更新的步长主要由 $(\hat{y}_i - y_i)$ 决定。

将均方差损失的梯度与二分类交叉熵损失的梯度放在一起比较一下就非常清晰了，如下：

$$\frac{\partial L_i}{\partial w}=(\hat{y}_i - y_i)x_i \qquad // \text{二分类交叉熵损失的梯度}$$

$$\frac{\partial L_i}{\partial w}=(\hat{y}_i - y_i)\sigma^{\prime}(z_i) x_i \qquad// \text{均方差损失的梯度}$$

可以看出两者之间仅差一个 $\sigma^{\prime}(z_i)$，所以有如下结论：

* 二分类交叉熵损失的梯度为期望输出与实际输出的差值，当距离目标点越远时，该差值越大，梯度越大；当距离目标点越近时，该差值越小，梯度越小；

* 均方差损失的梯度中由于包含了 $\sigma^{\prime}(z_i)$ 这一项，当距离目标点比较远时，梯度较小（趋于0）；当距离目标点较近时，梯度也较小（趋于0），不利于优化；
