# 二分类交叉熵损失梯度下降推导

## 1、问题描述

假设模型为单个神经元，单输入，单输出，二分类任务，使用sigmoid做二分类，则其前向传播过程为：

$$\begin{equation}z_i=wx_i+b\end{equation}$$

$$\begin{equation}\hat{y}_i=\sigma(z_i)\end{equation}$$

其损失为（关于损失函数原理见[分类任务损失函数的原理](../分类任务损失函数的原理/)）：

$$\begin{equation}L_i=-\Big[ y_i \log \hat{y}_i + (1-y_i)\log (1-\hat{y}_i) \Big]\end{equation}$$

$$\begin{equation}L=\frac{1}{N} \sum_{i=1}^N L_i\end{equation}$$

符号说明：

* $N$：表示样本数量；
* $x_i$：表示第$i$条样本的输入；
* $y_i$：表示第$i$条样本的期望输出；
* $\hat{y_i}$：表示第$i$条样本的模型输出；
* $z_i$：表示第$i$条样本只经过权重矩阵，未经过激活函数的中间结果；
* $L_i$：表示第$i$条样本的损失；
* $L$：表示所有$N$条样本的损失；
* $w$和$b$：表示权重参数；

## 2、求导

先使用链式求导法则求解 $\frac{\partial L_i}{\partial w}$，求解过程如下所示：

$$\begin{equation}\begin{split}
\frac{\partial L_i}{\partial w} &= \frac{\partial L_i}{\partial \hat{y}_i} \frac{\partial \hat{y}_i}{\partial z_i} \frac{\partial z_i}{\partial w} \\
&= -(\frac{y_i}{\hat{y}_i} - \frac{1-y_i}{1-\hat{y}_i}) \sigma^{\prime}(z_i) x_i \\
&= -(\frac{y_i}{\hat{y}_i} - \frac{1-y_i}{1-\hat{y}_i}) \sigma(z_i)(1-\sigma(z_i)) x_i \\
&= -(\frac{y_i}{\hat{y}_i} - \frac{1-y_i}{1-\hat{y}_i}) \hat{y}_i(1-\hat{y}_i) x_i \\
&= -[y_i(1-\hat{y}_i) - \hat{y}_i(1-y_i)]x_i \\
&= - (y_i - y_i \hat{y}_i - \hat{y}_i + y_i \hat{y}_i) x_i \\
&= (\hat{y}_i - y_i) x_i
\end{split}\end{equation}$$

再求解 $\frac{\partial L_i}{\partial b}$，求解过程如下所示：

$$\begin{equation}\begin{split}
\frac{\partial L_i}{\partial b}
&= \frac{\partial L_i}{\partial \hat{y}_i} \frac{\partial \hat{y}_i}{\partial z_i} \frac{\partial z_i}{\partial b} \\
&= -(\frac{y_i}{\hat{y}_i} - \frac{1-y_i}{1-\hat{y}_i}) \sigma^{\prime}(z_i) \\
&= -(\frac{y_i}{\hat{y}_i} - \frac{1-y_i}{1-\hat{y}_i}) \sigma(z_i)(1-\sigma(z_i)) \\
&= -(\frac{y_i}{\hat{y}_i} - \frac{1-y_i}{1-\hat{y}_i}) \hat{y}_i(1-\hat{y}_i) \\
&= -[y_i(1-\hat{y}_i) - \hat{y}_i(1-y_i)] \\
&= - (y_i - y_i \hat{y}_i - \hat{y}_i + y_i \hat{y}_i) \\
&= (\hat{y}_i - y_i)
\end{split}\end{equation}$$

> 说明：上述推导中使用到了sigmoid的求导公式，若 $f(z)=\frac{1}{1+e^{-z}}$，则有：$f^{\prime}(z) = f(z)(1 - f(z))$

至此求解出了两个导数：

$$\begin{equation}\begin{cases}
\frac{\partial L_i}{\partial w}=(\hat{y}_i - y_i) x_i \\
\frac{\partial L_i}{\partial b}=(\hat{y}_i - y_i)
\end{cases}\end{equation}$$

## 3、梯度下降

求解出导数之后，权重值的更新比较简单，如下：

$$\begin{equation}w=w - \eta \frac{\partial L}{\partial w}=w - \eta (\frac{1}{N} \sum_{i=1}^N \frac{\partial L_i}{\partial w})\end{equation}$$

$$\begin{equation}b=b - \eta \frac{\partial L}{\partial b}=b - \eta (\frac{1}{N} \sum_{i=1}^N \frac{\partial L_i}{\partial b})\end{equation}$$

## 4、总结

本文主要是对二分类交叉熵损失进行求导，以及给出其梯度下降的过程；

## Reference

太早了，不记得了...
