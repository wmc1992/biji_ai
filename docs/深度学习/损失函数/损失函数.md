# 损失函数

## 一、本文整体结构

* 总共几个损失函数？

    * 均方差损失（回归问题）
    * 二分类交叉熵损失（分类问题）
    * 多分类交叉熵损失（分类问题）

* 分类任务的损失函数的原理；

* 各个损失函数的公式、求导过程、分析、对比：

    * 均方差损失；
    * 二分类交叉熵损失；
    * 多分类交叉熵损失；

## 二、总共几个损失函数

## 三、分类任务的损失函数的原理

### 3.1 从似然函数推导出损失函数的通用形式

从极大化似然函数的角度出发，我们希望极大化如下似然函数。公式中的符号 $N$ 表示样本总数。

$$\begin{equation}L(\theta)=\log \prod_{i=1}^N p(x_i; \theta) = \sum_{i=1}^N \log p(x_i;\theta)\end{equation}$$

对于上式，把分类问题的类别总数 $C$ 添加进去之后得到：

$$\begin{equation}L(\theta)=\sum_{i=1}^N \sum_{j=1}^C \log p( \text{样本为}j\text{类别} | x_i;\theta)\end{equation}$$

分类问题在对label进行编码时都是使用one hot编码，所以 $y$ 仅能取值0或1，即 $y\in\{0,1\}$；为了后面方便描述，不妨记样本为 $k$ 类别时，$y=1$；样本为其他类别时，$y=0$，即：

$$\begin{equation}
y=\begin{cases}
1, &\text{样本为}k\text{类别}\\
0, &\text{样本为}k\text{以外的类别}
\end{cases}
\end{equation}$$

将公式（3）代入到公式（2）中得到：

$$\begin{equation}
\begin{split}
L(\theta)&=\sum_{i=1}^N \sum_{j=1}^C \log p( \text{样本为}j\text{类别} | x_i;\theta) \\
&=\begin{cases}
\sum_{i=1}^N \log p(x_i; \theta), &\text{样本为}k\text{类别，即}j=k\\
\sum_{i=1}^N \log \big[ 1 - p(x_i; \theta) \big], &\text{样本为}k\text{以外的类别，即}j \neq k
\end{cases} \\
&=\sum_{i=1}^N \sum_{j=1}^C y_i \log p(x_i;\theta)
\end{split}
\end{equation}$$

### 3.2 从KL散度推导出损失函数的通用形似


