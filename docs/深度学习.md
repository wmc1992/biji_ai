# 深度学习

## 一、损失函数

#### 1、[KL散度](./损失函数/KL散度/)

* 1）KL散度的定义
* 2）在机器学习中KL散度的作用
* 3）熵、KL散度、交叉熵
* 4）机器学习中为什么多用交叉熵而不是KL散度
* 5）KL散度的性质

#### 2、[损失函数](./损失函数/损失函数/)

* 1）本文整体结构
* 2）总共几个损失函数
* 3）分类任务的损失函数的原理
    * 3.1 从似然函数推导出损失函数的通用形式
    * 3.2 从KL散度推导出损失函数的通用形似

## 二、正则化

#### 1、[正则化综述](./正则化/正则化综述/)

* 1）参数约束正则化：L1和L2正则化
* 2）Dropout
* 3）数据增强（EDA）
* 4）早停（early stopping）

#### 2、[L1和L2正则化](./正则化/L1和L2正则化/)

* 1）L2正则
* 2）L1正则
* 3）Lasso回归与岭回归
    * 3.1 Lasso回归与岭回归的定义
    * 3.2 L1为何能做特征筛选
* 4）带有L2正则的目标函数的求解
    * 4.1 公式
    * 4.2 Pytorch中L2的实现
        * 4.2.1 调用方式
        * 4.2.2 源码
* 5）带有L1正则的目标函数的求解
    * 5.1 优化问题
    * 5.2 坐标轴下降法
    * 5.3 求解 $\arg \min_{\theta_l} J$
        * 5.3.1 问题描述
        * 5.3.2 求导
        * 5.3.3 令导数等于0求解 $\theta_l$
        * 5.3.4 伪代码实现

## 三、Normalize

#### 1、[Batch Norm](./Normalize/batch_normalize/)

* 1）BN具体的操作步骤
* 2）BN是为了解决什么问题
    * 2.1 ICS问题（Internal Covariate Shift）
    * 2.2 ICS会导致的问题
* 3）在BN出现前，是如何解决上述问题的
    * 3.1 白化操作
    * 3.2 白化操作的缺点
* 4）BN的作用与优势

#### 2、[Layer Norm](./Normalize/layer_normalize/)

* 1）LN的具体操作步骤
* 2）LN 为了解决什么问题
* 3）LN 出现之前是如何解决上述问题的
* 4）LN 的优势
* 5）LN效果测试代码
