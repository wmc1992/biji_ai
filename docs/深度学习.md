# 深度学习

## 一、损失函数

* [KL散度](./损失函数/KL散度/)
    * 1、KL散度的定义
    * 2、在机器学习中KL散度的作用
    * 3、熵、KL散度、交叉熵
    * 4、机器学习中为什么多用交叉熵而不是KL散度
    * 5、KL散度的性质

* [损失函数](./损失函数/损失函数/)
    * 一、本文整体结构
    * 二、总共几个损失函数
    * 三、分类任务的损失函数的原理
        * 3.1 从似然函数推导出损失函数的通用形式
        * 3.2 从KL散度推导出损失函数的通用形似

## 二、正则化

* [正则化](./正则化/正则化/)
    * 1、L2正则
    * 2、L1正则
    * 3、Lasso回归与岭回归
        * 3.1 Lasso回归与岭回归的定义
        * 3.2 L1为何能做特征筛选
    * 4、带有L2正则的目标函数的求解
        * 4.1 公式
        * 4.2 Pytorch中L2的实现
            * 4.2.1 调用方式
            * 4.2.2 源码
    * 5、带有L1正则的目标函数的求解
        * 5.1 优化问题
        * 5.2 坐标轴下降法
        * 5.3 求解 $\arg \min_{\theta_l} J$
            * 5.3.1 问题描述
            * 5.3.2 求导
            * 5.3.3 令导数等于0求解 $\theta_l$
            * 5.3.4 伪代码实现

## 三、Normalize

* [Batch Norm](./Normalize/batch_normalize/)
    * 1、BN具体的操作步骤
    * 2、BN是为了解决什么问题
        * 2.1 ICS问题（Internal Covariate Shift）
        * 2.2 ICS会导致的问题
    * 3、在BN出现前，是如何解决上述问题的
        * 3.1 白化操作
        * 3.2 白化操作的缺点
    * 4、BN的作用与优势

* [Layer Norm](./Normalize/layer_normalize/)
    * 1、LN的具体操作步骤
    * 2、LN 为了解决什么问题
    * 3、LN 出现之前是如何解决上述问题的
    * 4、LN 的优势5、LN效果测试代码
