# 深度学习

## 一、基础知识

### 1、损失函数

#### 1.1 [KL散度](./深度学习/损失函数/KL散度/)

* 1）KL散度的定义
* 2）在机器学习中KL散度的作用
* 3）熵、KL散度、交叉熵
* 4）机器学习中为什么多用交叉熵而不是KL散度
* 5）KL散度的性质

#### 1.2 [分类任务损失函数的原理](./深度学习/损失函数/分类任务损失函数的原理/)

* 1）分类任务的符号说明
* 2）从KL散度推导出损失函数
    * 2.1 推导二分类的损失函数
    * 2.2 推导多分类的损失函数
* 3）从似然函数推导出损失函数
    * 3.1 推导二分类的损失函数
    * 3.2 推导多分类的损失函数
* 4）总结

#### 1.3 [Softmax函数求导](./深度学习/损失函数/Softmax函数求导/)

* 1）问题描述
* 2）对softmax求导
* 3）总结

#### 1.4 [二分类交叉熵损失梯度下降推导](./深度学习/损失函数/二分类交叉熵损失梯度下降推导/)

* 1）问题描述
* 2）求导
* 3）梯度下降
* 4）总结

#### 1.5 [多分类交叉熵损失求导](./深度学习/损失函数/多分类交叉熵损失求导/)

* 1）前向传播过程中的符号定义
* 2）损失函数中的符号定义
* 3）求导
    * 3.1 总体分析
    * 3.2 求导
    * 3.3 最后
* 4）总结

#### 1.6 [均方差损失与交叉熵损失异同]()

### 2、正则化

#### 2.1 [正则化综述](./深度学习/正则化/正则化综述/)

* 1）参数约束正则化：L1和L2正则化
* 2）Dropout
* 3）数据增强（EDA）
* 4）早停（early stopping）

#### 2.2 [L1和L2正则化](./深度学习/正则化/L1和L2正则化/)

* 1）L2正则
* 2）L1正则
* 3）Lasso回归与岭回归
    * 3.1 Lasso回归与岭回归的定义
    * 3.2 L1为何能做特征筛选
* 4）带有L2正则的目标函数的求解
    * 4.1 公式
    * 4.2 Pytorch中L2的实现
        * 4.2.1 调用方式
        * 4.2.2 源码
* 5）带有L1正则的目标函数的求解
    * 5.1 优化问题
    * 5.2 坐标轴下降法
    * 5.3 求解 $\arg \min_{\theta_l} J$
        * 5.3.1 问题描述
        * 5.3.2 求导
        * 5.3.3 令导数等于0求解 $\theta_l$
        * 5.3.4 伪代码实现

### 3、Normalize

#### 3.1 [Batch Norm](./深度学习/Normalize/batch_normalize/)

* 1）BN具体的操作步骤
* 2）BN是为了解决什么问题
    * 2.1 ICS问题（Internal Covariate Shift）
    * 2.2 ICS会导致的问题
* 3）在BN出现前，是如何解决上述问题的
    * 3.1 白化操作
    * 3.2 白化操作的缺点
* 4）BN的作用与优势

#### 3.2 [Layer Norm](./深度学习/Normalize/layer_normalize/)

* 1）LN的具体操作步骤
* 2）LN 为了解决什么问题
* 3）LN 出现之前是如何解决上述问题的
* 4）LN 的优势
* 5）LN效果测试代码

## 二、自然语言处理（NLP）
